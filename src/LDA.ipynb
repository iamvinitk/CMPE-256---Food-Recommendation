{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>...</th>\n",
       "      <th>sugar</th>\n",
       "      <th>sodium</th>\n",
       "      <th>protein</th>\n",
       "      <th>saturated_fat</th>\n",
       "      <th>carbohydrates</th>\n",
       "      <th>food_types</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492</td>\n",
       "      <td>20636</td>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>this worked very well and is easy. i used not ...</td>\n",
       "      <td>20</td>\n",
       "      <td>56824</td>\n",
       "      <td>2002-10-27</td>\n",
       "      <td>30-minutes-or-less, time-to-make, course, main...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.8553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8204</td>\n",
       "      <td>64566</td>\n",
       "      <td>2005-09-02</td>\n",
       "      <td>4</td>\n",
       "      <td>very good</td>\n",
       "      <td>40</td>\n",
       "      <td>166019</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>60-minutes-or-less, time-to-make, main-ingredi...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Non-veg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.4927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28657</td>\n",
       "      <td>64566</td>\n",
       "      <td>2005-12-22</td>\n",
       "      <td>5</td>\n",
       "      <td>better than the real</td>\n",
       "      <td>40</td>\n",
       "      <td>166019</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>60-minutes-or-less, time-to-make, main-ingredi...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Non-veg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36365</td>\n",
       "      <td>64566</td>\n",
       "      <td>2006-09-26</td>\n",
       "      <td>5</td>\n",
       "      <td>absolutely awesome i was speechless when i tri...</td>\n",
       "      <td>40</td>\n",
       "      <td>166019</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>60-minutes-or-less, time-to-make, main-ingredi...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Non-veg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.6590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20197</td>\n",
       "      <td>64566</td>\n",
       "      <td>2007-03-09</td>\n",
       "      <td>5</td>\n",
       "      <td>these taste absolutely wonderful my son in law...</td>\n",
       "      <td>40</td>\n",
       "      <td>166019</td>\n",
       "      <td>2005-08-24</td>\n",
       "      <td>60-minutes-or-less, time-to-make, main-ingredi...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Non-veg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.8908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  recipe_id        date  rating  \\\n",
       "0      492      20636  2002-12-01       4   \n",
       "1     8204      64566  2005-09-02       4   \n",
       "2    28657      64566  2005-12-22       5   \n",
       "3    36365      64566  2006-09-26       5   \n",
       "4    20197      64566  2007-03-09       5   \n",
       "\n",
       "                                              review  minutes  contributor_id  \\\n",
       "0  this worked very well and is easy. i used not ...       20           56824   \n",
       "1                                          very good       40          166019   \n",
       "2                               better than the real       40          166019   \n",
       "3  absolutely awesome i was speechless when i tri...       40          166019   \n",
       "4  these taste absolutely wonderful my son in law...       40          166019   \n",
       "\n",
       "    submitted                                               tags  n_steps  \\\n",
       "0  2002-10-27  30-minutes-or-less, time-to-make, course, main...        5   \n",
       "1  2005-08-24  60-minutes-or-less, time-to-make, main-ingredi...       10   \n",
       "2  2005-08-24  60-minutes-or-less, time-to-make, main-ingredi...       10   \n",
       "3  2005-08-24  60-minutes-or-less, time-to-make, main-ingredi...       10   \n",
       "4  2005-08-24  60-minutes-or-less, time-to-make, main-ingredi...       10   \n",
       "\n",
       "   ... sugar sodium  protein  saturated_fat  carbohydrates  food_types  neg  \\\n",
       "0  ...  39.0    5.0      4.0           11.0            5.0     Healthy  0.0   \n",
       "1  ...  40.0   37.0     78.0            4.0           10.0     Non-veg  0.0   \n",
       "2  ...  40.0   37.0     78.0            4.0           10.0     Non-veg  0.0   \n",
       "3  ...  40.0   37.0     78.0            4.0           10.0     Non-veg  0.0   \n",
       "4  ...  40.0   37.0     78.0            4.0           10.0     Non-veg  0.0   \n",
       "\n",
       "     neu    pos  compound  \n",
       "0  0.598  0.402    0.8553  \n",
       "1  0.238  0.762    0.4927  \n",
       "2  0.508  0.492    0.4404  \n",
       "3  0.883  0.117    0.6590  \n",
       "4  0.675  0.325    0.8908  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = pd.read_csv('../dataset/preprocessed_data.csv')\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           arriba   baked winter squash mexican style\n",
       "1                     a bit different  breakfast pizza\n",
       "2                            all in the kitchen  chili\n",
       "3                                   alouette  potatoes\n",
       "4                   amish  tomato ketchup  for canning\n",
       "                              ...                     \n",
       "231632                                     zydeco soup\n",
       "231633                                zydeco spice mix\n",
       "231634                       zydeco ya ya deviled eggs\n",
       "231635          cookies by design   cookies on a stick\n",
       "231636    cookies by design   sugar shortbread cookies\n",
       "Name: name, Length: 231637, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = pd.read_csv('../dataset/RAW_recipes.csv')\n",
    "interactions = pd.read_csv('../dataset/RAW_interactions.csv')\n",
    "recipes['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sanikakarwa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [arriba, baked, winter, squash, mexican, style]\n",
       "1                      [bit, different, breakfast, pizza]\n",
       "2                                        [kitchen, chili]\n",
       "3                                    [alouette, potatoes]\n",
       "4                       [amish, tomato, ketchup, canning]\n",
       "                               ...                       \n",
       "231632                                     [zydeco, soup]\n",
       "231633                               [zydeco, spice, mix]\n",
       "231634                    [zydeco, ya, ya, deviled, eggs]\n",
       "231635                  [cookies, design, cookies, stick]\n",
       "231636      [cookies, design, sugar, shortbread, cookies]\n",
       "Name: processed_text, Length: 231637, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "recipes['processed_text'] = recipes['name'].apply(preprocess_text)\n",
    "recipes['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 5\u001b[0m bow_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(recipes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m bow_matrix\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:239\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Convert text data into bag-of-words representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(recipes['name'])\n",
    "bow_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
